{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pr1729p/deep_learning/blob/main/gpt_architecture_book_w_n_p.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SJNCV-kI28et"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "xFb0wG_g3KrT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "NPUF9vKC4B-T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the text\n",
        "book_text = open('book-war-and-peace.txt', 'r', encoding = 'utf-8').read()"
      ],
      "metadata": {
        "id": "h2SsjOF84b-6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#few lines from the text\n",
        "print(book_text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGXhf2WW4cAX",
        "outputId": "819d2a90-9a02-43e1-c92b-85de2443c403"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER I\n",
            "\n",
            "\"Well, Prince, so Genoa and Lucca are now just family estates of the\n",
            "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
            "if you still try to defend the infamies and horrors perpetrated by that\n",
            "Antichrist--I really believe he is Antichrist--I will have nothing more\n",
            "to do with you and you are no longer my friend, no longer my 'faithful\n",
            "slave,' as you call yourself! But how do you do? I see I have frightened\n",
            "you--sit down and tell me all the news.\"\n",
            "\n",
            "It was in July, 180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#total number of characters in the text\n",
        "print(len(book_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgUEIrE94cD3",
        "outputId": "7c8d7cf3-2dc3-4b26-b954-faae91460987"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3202303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#number of distinct characters in the text\n",
        "vocab = sorted(list(set(book_text)))\n",
        "vocab_size  = len(vocab)\n",
        "all_vocab = ''.join(vocab)\n",
        "print(all_vocab)\n",
        "print('vocab_size:', vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duwlft_z4cFf",
        "outputId": "832b8045-eefe-43f2-f15f-e173e226c8c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzàäéê\n",
            "vocab_size: 82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dictionary to map char to int\n",
        "char_to_int = {c:i for i,c in enumerate(vocab)}\n",
        "int_to_char = {i:c for c,i in char_to_int.items()}\n",
        "print('char_to_int:',char_to_int)\n",
        "print('int_to_char:', int_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgn4eFsm4cJK",
        "outputId": "6519ac14-e8d7-4343-975a-f7ad71bb2a93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "char_to_int: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, 'à': 78, 'ä': 79, 'é': 80, 'ê': 81}\n",
            "int_to_char: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: '*', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '=', 25: '?', 26: 'A', 27: 'B', 28: 'C', 29: 'D', 30: 'E', 31: 'F', 32: 'G', 33: 'H', 34: 'I', 35: 'J', 36: 'K', 37: 'L', 38: 'M', 39: 'N', 40: 'O', 41: 'P', 42: 'Q', 43: 'R', 44: 'S', 45: 'T', 46: 'U', 47: 'V', 48: 'W', 49: 'X', 50: 'Y', 51: 'Z', 52: 'a', 53: 'b', 54: 'c', 55: 'd', 56: 'e', 57: 'f', 58: 'g', 59: 'h', 60: 'i', 61: 'j', 62: 'k', 63: 'l', 64: 'm', 65: 'n', 66: 'o', 67: 'p', 68: 'q', 69: 'r', 70: 's', 71: 't', 72: 'u', 73: 'v', 74: 'w', 75: 'x', 76: 'y', 77: 'z', 78: 'à', 79: 'ä', 80: 'é', 81: 'ê'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining an encode and decode lambda functions\n",
        "encode = lambda s: [char_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join(int_to_char[i] for i in l)\n",
        "print(encode(\"This is my code.\\\n",
        "It is for practice\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joOJwf1T4cKs",
        "outputId": "71f9b266-5edd-4e49-ede7-767f71be16e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 59, 60, 70, 1, 60, 70, 1, 64, 76, 1, 54, 66, 55, 56, 10, 34, 71, 1, 60, 70, 1, 57, 66, 69, 1, 67, 69, 52, 54, 71, 60, 54, 56]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1729)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzzE2GdmuCpN",
        "outputId": "e980bf7c-5dc5-4159-fed3-d8db494bc3fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79ffb5998830>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the entire text into encoded tensor\n",
        "data = torch.tensor(encode(book_text), dtype = torch.long)"
      ],
      "metadata": {
        "id": "sBj1-f1K4cOn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dividing the text into training and validation dataset\n",
        "n = int(0.85*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "NRri9hDi4cQU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters -- all here\n",
        "\n",
        "batch_size = 16     #--defines the number of batches which run parallely and independently\n",
        "block_size = 64    # -- also known as max context length which will be fed into the transformer\n",
        "\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # if gpu is available\n",
        "\n",
        "max_iters = 3000\n",
        "eval_iters = 200\n",
        "\n",
        "\n",
        "n_embd = 64    # dimension of embeddings\n",
        "n_head = 4     # number of heads in multihead attention\n",
        "n_layer = 4\n",
        "dropout_prob = 0\n",
        "\n",
        "eval_interval = 100"
      ],
      "metadata": {
        "id": "1I2cCfKbt97s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_batch function-- randomly selects the batch_size of index and from each of these indices x contains the context and y contains their respectives targets\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data)- block_size,(batch_size,))\n",
        "  x = torch.stack([data[i: i+ block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1: i+ block_size +1] for i in ix])\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  return x,y\n"
      ],
      "metadata": {
        "id": "4JFpPr7d4cUC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#estimating loss\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for x in range(eval_iters):\n",
        "      X,Y = get_batch(split)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[x] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "GchztTCR4cVq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#self attention with single head\n",
        "class Head(nn.Module):\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "    # key, value, query\n",
        "    self.key = nn.Linear(n_embd, head_size,bias = False)\n",
        "    self.query = nn.Linear(n_embd, head_size,bias = False)\n",
        "    self.value = nn.Linear(n_embd, head_size,bias = False)\n",
        "\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)     #B,T,C-- head_size\n",
        "    q = self.query(x)    #B,T,C-- head_size\n",
        "    wgt = q @ k.transpose(-2,-1) * C**-0.5     #B,T,head_size  @ B,head_size,T ---> B, T, T\n",
        "    wgt = wgt.masked_fill(self.tril[:T, :T] == 0, float('-inf'))    # masking the future tokens -- auto regressive\n",
        "    wgt = F.softmax(wgt, dim =-1)\n",
        "\n",
        "    wgt = self.dropout(wgt)\n",
        "    v = self.value(x)\n",
        "    out = wgt @ v  #B,T,T @ B,T,C--> B,T,C\n",
        "    return out"
      ],
      "metadata": {
        "id": "iRSkSEIa4cZ8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multihead attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([p(x) for p in self.heads], dim = -1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "6FNuWPSf4cdI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embd, n_embd),\n",
        "        nn.Dropout(dropout_prob),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n"
      ],
      "metadata": {
        "id": "4ZX5ZRIO4cha"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining block of a transformer\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)  #Layer Normalization\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x+ self.sa(self.ln1(x))   # residual layer\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "fZxxVRNo4ciz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    B,T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "\n",
        "    x= tok_emb +pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      logits, loss = self(idx_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, dim = -1)\n",
        "      idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "      idx = torch.cat((idx, idx_next), dim = 1)\n",
        "\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "NIGMDTTl4clw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval == 0 or iter == max_iters -1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter} : train loss {losses['train']: .4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb,yb = get_batch('train')\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "context = torch.zeros((1,1), dtype = torch.long, device = device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa2jr6wv4cnT",
        "outputId": "9a757af7-8c28-4067-e646-c8419455ea54"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21397 M parameters\n",
            "step 0 : train loss  4.5884, val loss 4.5849\n",
            "step 100 : train loss  2.6250, val loss 2.6099\n",
            "step 200 : train loss  2.5131, val loss 2.5121\n",
            "step 300 : train loss  2.4500, val loss 2.4497\n",
            "step 400 : train loss  2.4015, val loss 2.4053\n",
            "step 500 : train loss  2.3504, val loss 2.3485\n",
            "step 600 : train loss  2.2842, val loss 2.2882\n",
            "step 700 : train loss  2.2320, val loss 2.2295\n",
            "step 800 : train loss  2.1649, val loss 2.1756\n",
            "step 900 : train loss  2.1167, val loss 2.1293\n",
            "step 1000 : train loss  2.0640, val loss 2.0757\n",
            "step 1100 : train loss  2.0281, val loss 2.0364\n",
            "step 1200 : train loss  1.9923, val loss 2.0066\n",
            "step 1300 : train loss  1.9581, val loss 1.9723\n",
            "step 1400 : train loss  1.9428, val loss 1.9453\n",
            "step 1500 : train loss  1.9057, val loss 1.9184\n",
            "step 1600 : train loss  1.8923, val loss 1.9048\n",
            "step 1700 : train loss  1.8708, val loss 1.8729\n",
            "step 1800 : train loss  1.8412, val loss 1.8610\n",
            "step 1900 : train loss  1.8302, val loss 1.8440\n",
            "step 2000 : train loss  1.8114, val loss 1.8144\n",
            "step 2100 : train loss  1.7927, val loss 1.8100\n",
            "step 2200 : train loss  1.7800, val loss 1.7949\n",
            "step 2300 : train loss  1.7681, val loss 1.7849\n",
            "step 2400 : train loss  1.7583, val loss 1.7858\n",
            "step 2500 : train loss  1.7543, val loss 1.7586\n",
            "step 2600 : train loss  1.7224, val loss 1.7436\n",
            "step 2700 : train loss  1.7196, val loss 1.7340\n",
            "step 2800 : train loss  1.7027, val loss 1.7217\n",
            "step 2900 : train loss  1.6841, val loss 1.6988\n",
            "step 2999 : train loss  1.6788, val loss 1.7071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens = 2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnIOdTITjLat",
        "outputId": "836309e8-378f-47e7-b349-dff98eb8335b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bmody unpaity:\n",
            "\"That yesit juests. She vidorly down thefe been\n",
            "mastily you, to offiired that he prove to joung with the lat fined serl\n",
            "teneration, why I that etyening, over why kremoy is the stubed so fix with came\n",
            "will a take a not unnd, preased) the gots.\n",
            "\n",
            "\n",
            "The warcives estainabby the Emperor bowing-losed.\"\n",
            "\n",
            "\"That opaiblity, the glesst. Prince Andrew avanyk imabout the most\n",
            "colyorser' leaing trettler of but the Emperousa husboken, ageaut having\n",
            "finfingshts will feen of the I penouse to she immant unearms to\n",
            "lifted in his dotomember's after. Vere up bevilly with a underroward\n",
            "lesterer began to peace, but Nare at aligholed nove infeersmiong\n",
            "to peopliby in that the ongates.\n",
            "\n",
            "\"Don'ter his exchider.\n",
            "\n",
            "\"Then, pattened Bogral shat the eman dry suriausk the Frenchina,\n",
            "Prince Verited hearfuard. Lelay, but assualtion's in smile heads\n",
            "in the heasts his shorcound regurg at reard cacchs, and twall all impartor and horen\n",
            "talkenlyt of came esto wark. Gening on it in whut lakeing of the\n",
            "Blanzagress wUrpirs.... He converges dayking looked his hy surritcimabity. It's! 1Lewdeoned. Hell\n",
            "tharl unciousess in fored, noparter, clumbllow of they afflaming? I of\n",
            "expribles. There gadll male armoank. Pierre toll what looking which\n",
            "was rights. But Andnow Pavlorred. At your Rostovnans you drowng. It.\n",
            "\n",
            "I that, the was talking hand of to sat bark-ounds admasts-found.\n",
            "\n",
            "\"Emantions, pusacelly will a 1remplaaws a wayound her cidgrage him, white hand\n",
            "din his young helvings, any Pierre his forget benfockmain face, from the fount\n",
            "and urrappently the hopram un too Timaffaters to his food and to\n",
            "its your ruddenly. Nichopolenk conving...\n",
            "\n",
            "\"Costorders cold sarm behind.,\"\n",
            "\n",
            "\"that my well the had sit, frontoder it, thy comman with her whaite out a opacaps ever\n",
            "with hers in frote me fall for the delpeased threm weartien at be\n",
            "toway of stownly, only his opend. The then deformed palsifyt him you\n",
            "haf wait greging and was are from\n",
            "the funglance worly have turned the cartain too 4maroting him)sely\n",
            "diride now to a ve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w5y0kuQ3lsqw"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}